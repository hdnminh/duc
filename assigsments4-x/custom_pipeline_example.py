#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
VÃ­ dá»¥ vá» cÃ¡ch láº¯p rÃ¡p má»™t pipeline suy luáº­n Stable Diffusion tá»« cÃ¡c thÃ nh pháº§n riÃªng biá»‡t:
VAE (Variational Autoencoder)
UNet
CLIP Text Encoder
Scheduler

"""

import torch
from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler
from transformers import CLIPTextModel, CLIPTokenizer
from PIL import Image
import argparse
from tqdm.auto import tqdm
import logging
import os
from datetime import datetime

# Thiáº¿t láº­p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("custom_pipeline.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("StableDiffusion")

def create_custom_pipeline(device="cuda"):
    """
    Láº¯p rÃ¡p cÃ¡c thÃ nh pháº§n cá»§a Stable Diffusion thÃ nh má»™t pipeline tÃ¹y chá»‰nh
    """
    logger.info("Äang táº£i cÃ¡c thÃ nh pháº§n cá»§a Stable Diffusion...")
    
    # 1. Táº£i mÃ´ hÃ¬nh VAE Ä‘á»ƒ mÃ£ hÃ³a/giáº£i mÃ£ biá»ƒu diá»…n tiá»m áº©n
    logger.info("Táº£i VAE model...")
    vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae")
    
    # 2. Táº£i tokenizer vÃ  text encoder Ä‘á»ƒ mÃ£ hÃ³a vÄƒn báº£n
    logger.info("Táº£i CLIP tokenizer vÃ  text encoder...")
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
    text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14")
    
    # 3. Táº£i mÃ´ hÃ¬nh UNet Ä‘á»ƒ táº¡o biá»ƒu diá»…n tiá»m áº©n cá»§a áº£nh
    logger.info("Táº£i UNet model...")
    unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet")
    
    # 4. Táº£i scheduler - á»Ÿ Ä‘Ã¢y chÃºng ta sá»­ dá»¥ng LMS thay vÃ¬ PNDM máº·c Ä‘á»‹nh
    logger.info("Táº£i LMS scheduler...")
    scheduler = LMSDiscreteScheduler.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="scheduler")
    
    # Chuyá»ƒn cÃ¡c mÃ´ hÃ¬nh Ä‘áº¿n thiáº¿t bá»‹
    logger.info(f"Chuyá»ƒn cÃ¡c mÃ´ hÃ¬nh Ä‘áº¿n thiáº¿t bá»‹: {device}")
    vae = vae.to(device)
    text_encoder = text_encoder.to(device)
    unet = unet.to(device)
    
    return vae, text_encoder, tokenizer, unet, scheduler

def create_latents(prompt, tokenizer, text_encoder, unet, height, width, batch_size, device, seed=None):
    """
    Táº¡o text embeddings vÃ  latents ban Ä‘áº§u
    """
    logger.info(f"Táº¡o latents vá»›i seed: {seed if seed is not None else 'random'}")
    
    if seed is not None:
        generator = torch.Generator(device=device).manual_seed(seed)
        logger.info(f"ÄÃ£ thiáº¿t láº­p seed: {seed}")
    else:
        generator = torch.Generator(device=device)
        logger.info("Sá»­ dá»¥ng seed ngáº«u nhiÃªn")
    
    # Táº¡o text embeddings
    logger.info("Tokenizing prompt...")
    text_input = tokenizer(
        prompt, 
        padding="max_length", 
        max_length=tokenizer.model_max_length, 
        truncation=True, 
        return_tensors="pt"
    )
    
    logger.info("Táº¡o text embeddings...")
    with torch.no_grad():
        text_embeddings = text_encoder(text_input.input_ids.to(device))[0]
    
    # Táº¡o unconditional embeddings cho classifier-free guidance
    logger.info("Táº¡o unconditional embeddings...")
    max_length = text_input.input_ids.shape[-1]
    uncond_input = tokenizer(
        [""] * batch_size, 
        padding="max_length", 
        max_length=max_length, 
        return_tensors="pt"
    )
    
    with torch.no_grad():
        uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]
    
    # GhÃ©p text embeddings
    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])
    
    # Táº¡o latents ngáº«u nhiÃªn ban Ä‘áº§u
    logger.info(f"Táº¡o latents vá»›i kÃ­ch thÆ°á»›c: {batch_size}x{unet.in_channels}x{height // 8}x{width // 8}")
    latents = torch.randn(
        batch_size, unet.in_channels, height // 8, width // ğŸ˜,
        generator=generator,
        device=device  # Specify the device when creating the tensor
    )
    
    return latents, text_embeddings, generator

def generate_image(
    prompt,
    vae, 
    text_encoder, 
    tokenizer, 
    unet, 
    scheduler,
    height=512,
    width=512,
    guidance_scale=7.5,
    num_inference_steps=50,
    seed=None,
    device="cuda",
    batch_size=1
):
    """
    Táº¡o áº£nh tá»« prompt sá»­ dá»¥ng pipeline tÃ¹y chá»‰nh
    """
    logger.info(f"Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh táº¡o áº£nh vá»›i tham sá»‘:")
    logger.info(f"- Prompt: '{prompt}'")
    logger.info(f"- KÃ­ch thÆ°á»›c: {width}x{height}")
    logger.info(f"- Guidance scale: {guidance_scale}")
    logger.info(f"- Sá»‘ bÆ°á»›c: {num_inference_steps}")
    logger.info(f"- Device: {device}")
    
    # 1. Táº¡o text embeddings vÃ  latents ban Ä‘áº§u
    latents, text_embeddings, generator = create_latents(
        prompt, tokenizer, text_encoder, unet, 
        height, width, batch_size, device, seed
    )
    
    # 2. CÃ i Ä‘áº·t scheduler vá»›i sá»‘ bÆ°á»›c suy luáº­n
    logger.info(f"CÃ i Ä‘áº·t scheduler vá»›i {num_inference_steps} bÆ°á»›c")
    scheduler.set_timesteps(num_inference_steps)
    
    # 3. Chuáº©n bá»‹ latents cho quÃ¡ trÃ¬nh khá»­ nhiá»…u
    latents = latents * scheduler.init_noise_sigma
    
    # 4. VÃ²ng láº·p khá»­ nhiá»…u
    logger.info(f"Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh khá»­ nhiá»…u...")
    for t in tqdm(scheduler.timesteps):
        # NhÃ¢n Ä‘Ã´i latents cho classifier-free guidance
        latent_model_input = torch.cat([latents] * 2)
        
        # Äiá»u chá»‰nh scale input theo timestep
        latent_model_input = scheduler.scale_model_input(latent_model_input, t)
        
        # Dá»± Ä‘oÃ¡n noise residual
        with torch.no_grad():
            noise_pred = unet(
                latent_model_input, 
                t, 
                encoder_hidden_states=text_embeddings
            ).sample
        
        # Thá»±c hiá»‡n classifier-free guidance
        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
        
        # Khá»­ nhiá»…u: bÆ°á»›c t -> t-1
        latents = scheduler.step(noise_pred, t, latents).prev_sample
    
    # 5. Scale vÃ  giáº£i mÃ£ biá»ƒu diá»…n tiá»m áº©n
    logger.info("Giáº£i mÃ£ latents thÃ nh áº£nh...")
    latents = 1 / 0.18215 * latents
    
    with torch.no_grad():
        image = vae.decode(latents).sample
    
    # 6. Chuyá»ƒn Ä‘á»•i tá»« tensor sang Ä‘á»‹nh dáº¡ng áº£nh PIL
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()
    images = (image * 255).round().astype("uint8")
    pil_images = [Image.fromarray(image) for image in images]
    logger.info("ÄÃ£ chuyá»ƒn Ä‘á»•i thÃ nh cÃ´ng tensor thÃ nh áº£nh PIL")
    
    return pil_images

def main():
    # PhÃ¢n tÃ­ch tham sá»‘ dÃ²ng lá»‡nh
    parser = argparse.ArgumentParser(description="Custom Stable Diffusion Pipeline")
    parser.add_argument("--prompt", type=str, default="a photo of an astronaut riding a horse on mars",
                       help="MÃ´ táº£ vÄƒn báº£n Ä‘á»ƒ táº¡o áº£nh")
    parser.add_argument("--height", type=int, default=512,
                       help="Chiá»u cao áº£nh")
    parser.add_argument("--width", type=int, default=512,
                       help="Chiá»u rá»™ng áº£nh")
    parser.add_argument("--steps", type=int, default=50,
                       help="Sá»‘ bÆ°á»›c suy luáº­n")
    parser.add_argument("--guidance", type=float, default=7.5,
                       help="Guidance scale")
    parser.add_argument("--seed", type=int, default=None,
                       help="Seed ngáº«u nhiÃªn Ä‘á»ƒ tÃ¡i táº¡o áº£nh")
    parser.add_argument("--output", type=str, default="outputs/custom_output.png",
                       help="TÃªn file káº¿t quáº£")
    
    args = parser.parse_args()
    
    # XÃ¡c Ä‘á»‹nh thiáº¿t bá»‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cpu":
        logger.warning("Cáº£nh bÃ¡o: Báº¡n Ä‘ang sá»­ dá»¥ng CPU, quÃ¡ trÃ¬nh sinh áº£nh sáº½ ráº¥t cháº­m!")
    
    # Táº¡o pipeline tÃ¹y chá»‰nh
    vae, text_encoder, tokenizer, unet, scheduler = create_custom_pipeline(device)
    
    # Sinh áº£nh
    images = generate_image(
        args.prompt,
        vae, 
        text_encoder, 
        tokenizer, 
        unet, 
        scheduler,
        height=args.height,
        width=args.width,
        guidance_scale=args.guidance,
        num_inference_steps=args.steps,
        seed=args.seed,
        device=device
    )
    
    # Táº¡o tÃªn file káº¿t quáº£ vá»›i guidance vÃ  steps
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        logger.info(f"ÄÃ£ táº¡o thÆ° má»¥c Ä‘áº§u ra: {output_dir}")
    
    filename = os.path.basename(args.output)
    name, ext = os.path.splitext(filename)
    
    # Táº¡o tÃªn file má»›i vá»›i guidance vÃ  steps
    new_filename = f"{name}_g{args.guidance}_s{args.steps}{ext}"
    output_path = os.path.join(output_dir, new_filename)
    
    # LÆ°u áº£nh
    images[0].save(output_path)
    logger.info(f"ÄÃ£ lÆ°u áº£nh táº¡i: {output_path}")

if _name_ == "_main_":
    logger.info("Khá»Ÿi Ä‘á»™ng pipeline...")
    main()
    logger.info("HoÃ n thÃ nh!")